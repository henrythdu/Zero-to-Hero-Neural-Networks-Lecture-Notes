# Zero-to-Hero-Neural-Networks-Lecture-Notes

# Neural Networks: Zero to Hero

This repo is for the video series by Andrej Karpathy covers neural network fundamentals, advanced techniques, and practical applications. The course is designed to help you build neural networks from scratch, starting with the basics of backpropagation and progressing to modern deep neural networks like GPT. The note is going to include my own comments to make certain points clearer.

## Course Overview

1. **Introduction to Backpropagation and Neural Networks**
   - Step-by-step explanation of backpropagation and training neural networks.
   - Assumes basic knowledge of Python and a vague recollection of calculus from high school.

2. **Language Modeling with makemore**
   - Building a bigram character-level language model.
   - Introducing `torch.Tensor` and its use in efficiently evaluating neural networks.
   - Focusing on model training, sampling, and loss evaluation.

3. **Multilayer Perceptron (MLP) Language Model**
   - Implementing an MLP character-level language model.
   - Basics of machine learning, including hyperparameters, evaluation, and overfitting.

4. **Activations, Gradients, and Batch Normalization**
   - Analyzing MLP internals, forward pass activations, and backward pass gradients.
   - Understanding pitfalls and diagnostic tools.
   - Introducing Batch Normalization.

5. **Becoming a Backprop Ninja**
   - Manual backpropagation through an MLP (with BatchNorm).
   - Building intuition around gradient flow and optimization.

## Prerequisites
- Solid programming skills (Python).
- Intro-level math (e.g., derivatives, Gaussian distribution).

## Additional Resources
- Explore the course materials on [Andrej Karpathy's website](https://karpathy.ai/zero-to-hero.html).


Source:
(1) Neural Networks: Zero To Hero - Karpathy. [https://karpathy.ai/zero-to-hero.html](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ).
